{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o_Uvegnidzq6",
    "outputId": "ad9d2fb6-a87e-48f4-a0f9-40b91a224980"
   },
   "outputs": [],
   "source": [
    "!pip install IPython\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jAM51Z7znq_V"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/Jiyoon52/LG_ES_RNN.git # colab 사용시"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9uXbP9sNnq_S"
   },
   "source": [
    "# [Recurrent Neural Networks - 회귀] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QL7l9L4Jnq_T"
   },
   "source": [
    "##### jupyter notebook 단축키\n",
    "\n",
    "- ctrl+enter: 셀 실행   \n",
    "- shift+enter: 셀 실행 및 다음 셀 이동   \n",
    "- alt+enter: 셀 실행, 다음 셀 이동, 새로운 셀 생성\n",
    "- a: 상단에 새로운 셀 만들기\n",
    "- b: 하단에 새로운 셀 만들기\n",
    "- dd: 셀 삭제(x: 셀 삭제)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fr9IowEpnq_U"
   },
   "source": [
    "### 1. 모듈 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9ueLs11Jnq_W"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import time\n",
    "import copy\n",
    "\n",
    "import random\n",
    "import matplotlib.pyplot as plt     \n",
    "\n",
    "# regression 평가지표\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    mape = np.mean(np.abs((y_true - y_pred)/y_true))*100\n",
    "    return mape\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KL9O-z5Pnq_Y"
   },
   "source": [
    "### 2. 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BIG9APzydzrE"
   },
   "outputs": [],
   "source": [
    "random_seed = 2022\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed_all(random_seed) # if use multi-GPU\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://cseweb.ucsd.edu/~yaq007/NASDAQ100_stock_data.html\n",
    "- 나스닥 100에 속한 81개 기업의 주가와 나스닥 100지수 값으로 구성\n",
    "- 2016년 7월 26일부터 2016년 12월 22일까지 수집 (105일)\n",
    "- 누락된 데이터에 대해 정제된 데이터\n",
    "- X: 81개 종목의 종가\n",
    "- y: NASDAQ 100 지수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y4t-KvVLnq_Z"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('/content/LG_ES_RNN/data/reg_nasdaq100.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qEXesS9Dnq_Z"
   },
   "source": [
    "#### 2.1 Data Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BKSKQHdqnq_a",
    "outputId": "6374ffaa-e456-416a-e6f1-53c5d8bf33e1"
   },
   "outputs": [],
   "source": [
    "data.head() # 상위 5개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e94ovbFlnq_a",
    "outputId": "ee313b06-3186-425e-b482-7f69ddee2185"
   },
   "outputs": [],
   "source": [
    "data.tail() # 하위 5개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gqZPizHfnq_b",
    "outputId": "08a8f339-faaa-46bf-fa64-08ed2a1f8a9b"
   },
   "outputs": [],
   "source": [
    "data.shape # 데이터 차원 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mkym7Av-nq_b",
    "outputId": "7f03588c-9ccb-4422-e284-68dcfe4ebf56"
   },
   "outputs": [],
   "source": [
    "data.describe() # 데이터 통계값 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q9L-CxIpdzrJ",
    "outputId": "1caa6a3f-a6d9-4464-a99d-53a9b7f35116"
   },
   "outputs": [],
   "source": [
    "x = data.drop(['NDX'], axis=1)\n",
    "y = data[['NDX']]\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, shuffle=False)\n",
    "\n",
    "print(f'x_train shape is {x_train.shape}')\n",
    "print(f'y_train shape is {y_train.shape}')\n",
    "print('-'*35)\n",
    "print(f'x_test shape is {x_test.shape}')\n",
    "print(f'y_test shape is {y_test.shape}')\n",
    "print('-'*35)\n",
    "print('x_train examples')\n",
    "x_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zspmGkH-dzrJ",
    "outputId": "15855b8c-55e9-4c64-c021-9e326dc23850"
   },
   "outputs": [],
   "source": [
    "x_scaler = MinMaxScaler()\n",
    "y_scaler = MinMaxScaler()\n",
    "\n",
    "columns_list = list(x_train.columns)\n",
    "x_train = x_scaler.fit_transform(x_train)\n",
    "x_test = x_scaler.transform(x_test)\n",
    "y_train = y_scaler.fit_transform(y_train)\n",
    "y_test = y_scaler.transform(y_test)\n",
    "\n",
    "y_train = y_train.squeeze()\n",
    "y_test = y_test.squeeze()\n",
    "\n",
    "print('x_train examples')\n",
    "pd.DataFrame(x_train, columns = columns_list).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cKo2rdPbdzrK"
   },
   "source": [
    "#### 2.2 Data Windowing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('/content/LG_ES_RNN/image/image8.JPG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('/content/LG_ES_RNN/image/image9.JPG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z1MEoma-dzrK"
   },
   "outputs": [],
   "source": [
    "def windowing_process(x, y, window_size, shift_size, last = True):\n",
    "    try:\n",
    "        x = x.reset_index(drop=True)\n",
    "        y = y.reset_index(drop=True)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    x_window = []\n",
    "    y_window = []\n",
    "    \n",
    "    for start_idx in range(0, x.shape[0] - window_size + 1, shift_size):\n",
    "        x_window.append(x[start_idx:start_idx + window_size])\n",
    "        \n",
    "        if last == True:\n",
    "            y_window.append(y[start_idx + window_size - 1])  # 마지막 index의 값을 사용하는 경우\n",
    "        \n",
    "        else:\n",
    "            y_window.append(np.mean(y[start_idx:start_idx + window_size]))  # window 내 값들의 평균\n",
    "    \n",
    "    x_window = np.array(x_window)\n",
    "    y_window = np.array(y_window)\n",
    "    \n",
    "    return x_window, y_window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OMtlqXl1dzrL",
    "outputId": "b7ab3891-62c6-4177-c228-63add6c9c78d"
   },
   "outputs": [],
   "source": [
    "x_train, y_train = windowing_process(x_train, y_train, 5, 1, True)\n",
    "x_test, y_test = windowing_process(x_test, y_test, 5, 1, True)\n",
    "\n",
    "print(f'x_train shape is {x_train.shape}')\n",
    "print(f'y_train shape is {y_train.shape}')\n",
    "print('-'*35)\n",
    "print(f'x_test shape is {x_test.shape}')\n",
    "print(f'y_test shape is {y_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pOINW5o9dzrL",
    "outputId": "6f4447a6-9924-4aba-ce6b-e2720c962f44"
   },
   "outputs": [],
   "source": [
    "n_train = int(0.8 * len(x_train))\n",
    "x_valid, y_valid = x_train[n_train:], y_train[n_train:]\n",
    "x_train, y_train = x_train[:n_train], y_train[:n_train]\n",
    "\n",
    "print(f'x_train shape is {x_train.shape}')\n",
    "print(f'y_train shape is {y_train.shape}')\n",
    "print('-'*35)\n",
    "print(f'x_train shape is {x_valid.shape}')\n",
    "print(f'y_train shape is {y_valid.shape}')\n",
    "print('-'*35)\n",
    "print(f'x_test shape is {x_test.shape}')\n",
    "print(f'y_test shape is {y_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zxG4ygKAnq_e"
   },
   "source": [
    "### 3. RNN Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8yuGdb_7dzrM"
   },
   "source": [
    "#### 3.1 Define the Model Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jmJd0HwPnq_f"
   },
   "outputs": [],
   "source": [
    "class RNN_model(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, bidirectional, device='cuda'):\n",
    "        super(RNN_model, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.num_directions = 2 if bidirectional == True else 1\n",
    "        self.device = device\n",
    "        \n",
    "        # recurrent layer 설정\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True, bidirectional=bidirectional)\n",
    "     \n",
    "        # bidirectional에 따른 fc layer 구축\n",
    "        # bidirectional 여부에 따라 hidden state의 shape가 달라짐 (True: 2 * hidden_size, False: hidden_size)\n",
    "        self.fc = nn.Linear(self.num_directions * hidden_size, 1)  # self.num_classes = 1로 설정된 구조\n",
    "        # self.fc = nn.Linear(self.num_directions * hidden_size, self.num_classes) --> classification\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # data dimension: (batch_size x input_size x seq_len) -> (batch_size x seq_len x input_size)로 변환\n",
    "        # x = torch.transpose(x, 1, 2)\n",
    "        \n",
    "        # initial hidden states 설정\n",
    "        h0 = torch.zeros(self.num_directions * self.num_layers, x.size(0), self.hidden_size).to(self.device)\n",
    "        \n",
    "        # RNN으로부터 output 도출\n",
    "        out, _ = self.rnn(x, h0)  # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
    "\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jDunc8mQdzrN"
   },
   "source": [
    "#### 3.2 Define The Training Testing Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IAt1AcZTdzrN"
   },
   "outputs": [],
   "source": [
    "class Train_Test():\n",
    "    def __init__(self,  train_loader, valid_loader, test_loader, input_size, device='cuda'): \n",
    "        self.train_loader = train_loader\n",
    "        self.valid_loader = valid_loader\n",
    "        self.test_loader = test_loader\n",
    "\n",
    "        self.input_size = input_size\n",
    "\n",
    "    def train(self, model, dataloaders, criterion, num_epochs, optimizer):\n",
    "        since = time.time() \n",
    "        \n",
    "        train_loss_history = []\n",
    "        val_loss_history = []\n",
    "\n",
    "        best_model_wts = copy.deepcopy(model.state_dict()) # 모델의 초기 Weight값 (각 Layer 별 초기 Weight값이 저장되어 있음)\n",
    "        best_loss = 999999999 # MSE는 작을수록 좋은 metric이므로, 초기 높은 값에서 갱신\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            if epoch == 0 or (epoch + 1) % 10 == 0:\n",
    "                print()\n",
    "                print('Epoch {}/{}'.format(epoch + 1, num_epochs))\n",
    "\n",
    "            # 각 epoch마다 순서대로 training과 validation을 진행\n",
    "            for phase in ['train', 'val']:\n",
    "                if phase == 'train':\n",
    "                    model.train()  # 모델을 training mode로 설정\n",
    "                else:\n",
    "                    model.eval()   # 모델을 validation mode로 설정\n",
    "\n",
    "                running_loss = 0.0\n",
    "                running_total = 0\n",
    "\n",
    "                # training과 validation 단계에 맞는 dataloader에 대하여 학습/검증 진행\n",
    "                for inputs, targets in dataloaders[phase]:\n",
    "                    inputs = inputs.to(device)\n",
    "                    targets = targets.to(device)\n",
    "                    # seq_lens = seq_lens.to(self.parameter['device'])\n",
    "                    \n",
    "                    # parameter gradients를 0으로 설정\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    # forward\n",
    "                    # training 단계에서만 gradient 업데이트 수행\n",
    "                    with torch.set_grad_enabled(phase == 'train'):\n",
    "\n",
    "                        # input을 model에 넣어 output을 도출한 후, loss를 계산함\n",
    "                        outputs = model(inputs)\n",
    "                        outputs = outputs.reshape(-1)\n",
    "                        \n",
    "                        loss = criterion(outputs, targets)\n",
    "\n",
    "                        # backward (optimize): training 단계에서만 수행\n",
    "                        if phase == 'train':\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "\n",
    "                    # batch별 loss를 축적함\n",
    "                    running_loss += loss.item() * inputs.size(0)\n",
    "                    running_total += targets.size(0)\n",
    "\n",
    "                # epoch의 loss 및 accuracy 도출\n",
    "                epoch_loss = running_loss / running_total\n",
    "                                    \n",
    "                if epoch == 0 or (epoch + 1) % 10 == 0:\n",
    "                    print('{} Loss: {:.4f}'.format(phase, epoch_loss))\n",
    "\n",
    "                # validation 단계에서 validation loss가 감소할 때마다 best model 가중치를 업데이트함\n",
    "                if phase == 'val' and epoch_loss < best_loss:\n",
    "                    best_loss = epoch_loss\n",
    "                    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                if phase == 'train':\n",
    "                    train_loss_history.append(epoch_loss)\n",
    "                elif phase == 'val':\n",
    "                    val_loss_history.append(epoch_loss)\n",
    "\n",
    "        # 전체 학습 시간 계산 (학습이 완료된 후)\n",
    "        time_elapsed = time.time() - since\n",
    "        print('\\nTraining complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "        print('Best val MSE: {:4f}'.format(best_loss))\n",
    "\n",
    "        # validation loss가 가장 낮았을 때의 best model 가중치를 불러와 best model을 구축함\n",
    "        model.load_state_dict(best_model_wts)\n",
    "        return model, train_loss_history, val_loss_history\n",
    "\n",
    "    def test(self, model, test_loader):\n",
    "        model.eval()   # 모델을 validation mode로 설정\n",
    "        \n",
    "        # test_loader에 대하여 검증 진행 (gradient update 방지)\n",
    "        with torch.no_grad():\n",
    "\n",
    "            preds = []\n",
    "            y_true = []\n",
    "            for inputs, targets in test_loader:\n",
    "                inputs = inputs.to(device)\n",
    "                targets = targets.to(device)\n",
    "\n",
    "                # forward\n",
    "                # input을 model에 넣어 output을 도출\n",
    "                pred = model(inputs)\n",
    "                \n",
    "                preds.extend(pred.detach().cpu().numpy())\n",
    "                y_true.extend(targets.detach().cpu().numpy())\n",
    "\n",
    "            preds = torch.tensor(preds).reshape(-1)\n",
    "            y_true = torch.tensor(y_true)\n",
    "            \n",
    "            mse = nn.MSELoss()(preds, y_true).item()\n",
    "            preds = preds.detach().cpu().numpy()\n",
    "        return preds, mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3gm2Pj3YdzrO"
   },
   "source": [
    "### 4. Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-_lrnlB0dzrO"
   },
   "source": [
    "#### 4.1 Hyperparameter Setting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- batch_size: 학습 및 검증에 사용할 배치의 크기\n",
    "- input_size: 변수 개수\n",
    "- hidden_size: 모델의 hidden dimension\n",
    "- num_layers: 모델의 layer 개수\n",
    "- bidirectional: 모델의 양방향성 여부\n",
    "- num_epochs: 학습할 epoch 횟수\n",
    "- device: 사용할 자원 선정\n",
    "- best_model_path: 모델 파라미터 저장 위치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vPS-uWzKdzrO"
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "input_size = x_train.shape[2]\n",
    "hidden_size = 64\n",
    "num_layers = 2\n",
    "bidirectional = True\n",
    "num_epochs = 200\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else 'cpu') \n",
    "best_model_path = '/content/LG_ES_RNN/ckpt/rnn_reg.pt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4-ZaVN0WdzrP"
   },
   "source": [
    "#### 4.2 Construct Data Loaders and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gA-Akza0dzrP"
   },
   "outputs": [],
   "source": [
    "datasets = []\n",
    "for dataset in [(x_train, y_train), (x_valid, y_valid), (x_test, y_test)]:\n",
    "    x_data = dataset[0]\n",
    "    y_data = dataset[1]\n",
    "    datasets.append(torch.utils.data.TensorDataset(torch.Tensor(x_data), torch.Tensor(y_data)))\n",
    "\n",
    "trainset, validset, testset = datasets[0], datasets[1], datasets[2]\n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(validset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3LKgDX1ydzrQ"
   },
   "outputs": [],
   "source": [
    "model = RNN_model(input_size, hidden_size, num_layers, bidirectional, device)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zC3cC0acdzrR"
   },
   "source": [
    "#### 4.3 Model Training and Save Weights(Parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g4DpkNX-dzrR"
   },
   "outputs": [],
   "source": [
    "dataloaders_dict = {'train': train_loader, 'val': valid_loader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CCJipGb-dzrR"
   },
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RjCVeP79dzrS",
    "outputId": "79d76fcf-2d70-4cb1-a538-7c9090bc662b"
   },
   "outputs": [],
   "source": [
    "trainer = Train_Test(train_loader, valid_loader, test_loader, input_size, device)\n",
    "best_model, train_loss_history, val_loss_history = trainer.train(model, dataloaders_dict, criterion, num_epochs, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "54r2ywcsdzrS"
   },
   "outputs": [],
   "source": [
    "torch.save(best_model.state_dict(), best_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o73bPzjldzrT"
   },
   "source": [
    "### 5. Model Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jeqAjHsIdzrV"
   },
   "source": [
    "#### 5.1 Loss History Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_m7bFWyDdzrV",
    "outputId": "ea0b00fa-3919-4a20-db86-4031ba2a0020"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "plt.title('Loss History')\n",
    "plt.plot(range(num_epochs), train_loss_history, c='blue', label='Train Loss')\n",
    "plt.plot(range(num_epochs), val_loss_history, c='red', label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LZHROIK8dzrW"
   },
   "source": [
    "#### 5.2 Load Model Weights(Parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CRfhOcP-dzrW",
    "outputId": "b418db97-3f42-4c85-f49f-26549f126d4e"
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(best_model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ddrl3HuhdzrW",
    "outputId": "0021b56b-7bc1-4f07-e7ac-cc068d41f15a"
   },
   "outputs": [],
   "source": [
    "y_pred, mse = trainer.test(model, test_loader)\n",
    "y_pred_inverse = y_scaler.inverse_transform(pd.DataFrame(y_pred))\n",
    "y_test_inverse = y_scaler.inverse_transform(pd.DataFrame(y_test))\n",
    "y_train_inverse = y_scaler.inverse_transform(pd.DataFrame(y_train))\n",
    "y_valid_inverse = y_scaler.inverse_transform(pd.DataFrame(y_valid))\n",
    "\n",
    "print('y prediction (raw)')\n",
    "print(y_pred)\n",
    "print('-'*35)\n",
    "print('y prediction (inverse minmax scaler)')\n",
    "print(y_pred_inverse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  R squared (결정계수 $({r})^{2}$) <br>\n",
    "$\\frac{SSR}{SST} = 1- \\frac{SSR}{SST}%$\n",
    "\n",
    "##### Mean Absolute Error (평균 절대 오차) <br>\n",
    "$\\frac{1}{n} \\sum_{i=1}^{n} |y_{i} - \\hat{y}_{i}|$\n",
    "\n",
    "###### Mean Squared Error (평균 제곱 오차) <br>\n",
    "$\\frac{1}{n} \\sum_{i=1}^{n} (y_{i} - \\hat{y}_{i})^{2}$   \n",
    "\n",
    "######  Root Mean Squared Error (제곱근 평균 제곱 오차) <br>\n",
    "$\\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_{i} - \\hat{y}_{i})^{2}}$\n",
    "\n",
    "##### Mean Absolute Percentage Error (평균 절대비 오차) <br>\n",
    "$\\sum_{i=1}^{n} |\\frac{y_{i} - \\hat{y}_{i}}{\\hat{y}_{i}}| *100\\%$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jy_kSLH5dzrX",
    "outputId": "0f50052a-1f19-46d1-b56e-80b82d3683d5"
   },
   "outputs": [],
   "source": [
    "def regression_report(y_true, y_pred):\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    mae = mean_absolute_error(y_true, y_pred) \n",
    "    mse = mean_squared_error(y_true, y_pred) \n",
    "    rmse = np.sqrt(mse)\n",
    "    mape = mean_absolute_percentage_error(y_true, y_pred)\n",
    "      \n",
    "    print('The regression reports are as follows:')\n",
    "    print('r2: ', round(r2,4))\n",
    "    print('MAE: ', round(mae,4))\n",
    "    print('MSE: ', round(mse,4))\n",
    "    print('RMSE: ', round(rmse,4))\n",
    "    print('MAPE: ', round(mape,4))\n",
    "    return r2, mae, mse, rmse, mape\n",
    "\n",
    "r2, mae, mse, rmse, mape = regression_report (y_test_inverse.flatten(), y_pred_inverse.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-smklPfTdzrX"
   },
   "outputs": [],
   "source": [
    "y_train_concat = np.concatenate([y_train_inverse, y_valid_inverse], axis=0)\n",
    "y_train = np.concatenate([y_train_concat, np.array([np.nan]*len(y_test_inverse)).reshape(len(y_test_inverse), -1)], axis=0)\n",
    "y_test = np.concatenate([np.array([np.nan]*len(y_train_concat)).reshape(len(y_train_concat), -1), y_test_inverse], axis=0)\n",
    "y_pred = np.concatenate([np.array([np.nan]*len(y_train_concat)).reshape(len(y_train_concat), -1), y_pred_inverse], axis=0)\n",
    "\n",
    "y_train = pd.DataFrame(y_train, columns = ['Train set'])\n",
    "y_test = pd.DataFrame(y_test, columns = ['Test set'])\n",
    "y_pred = pd.DataFrame(y_pred, columns = ['Predictions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HK4s1qgadzrX",
    "outputId": "5a5aa6ed-46ee-4bf4-ea42-f1f943ba7512"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,6))\n",
    "plt.title('RNN regression results', fontsize=12)\n",
    "plt.xlabel('Date', fontsize=12)\n",
    "plt.ylabel('NASDAQ 100 Index', fontsize=12)\n",
    "plt.axvline(x=len(y_train_concat), color='r', label='Start Prediction', ls='--')\n",
    "plt.plot(y_train['Train set'])\n",
    "plt.plot(y_test['Test set'])\n",
    "plt.plot(y_pred['Predictions'])\n",
    "plt.legend(['Start Prediction', 'Train set', 'Test set', 'Predictions'], loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gqBHwh5-nq_q"
   },
   "source": [
    "# EOD"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "2_RNN_regression.ipynb",
   "provenance": [
    {
     "file_id": "https://github.com/Jiyoon52/LG_ES_RNN/blob/main/2_RNN_regression.ipynb",
     "timestamp": 1656959224839
    },
    {
     "file_id": "https://github.com/Jiyoon52/LG_time_series_day02_dataset/blob/main/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%EC%8B%9C%EA%B3%84%EC%97%B4%EB%B6%84%EC%84%9D%202%ED%9A%8C%EC%B0%A8%20-%201.ipynb",
     "timestamp": 1642046886935
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "toc": {
   "base_numbering": "1",
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
