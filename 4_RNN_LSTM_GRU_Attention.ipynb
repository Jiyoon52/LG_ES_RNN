{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5XLVkiyQeSOZ",
    "outputId": "61bc3453-6a20-47ab-db28-8656d0e4428d"
   },
   "outputs": [],
   "source": [
    "!pip install IPython\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jAM51Z7znq_V"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/Jiyoon52/LG_ES_RNN.git # colab 사용시"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9uXbP9sNnq_S"
   },
   "source": [
    "# [Recurrent Neural Networks - 회귀비교실험] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QL7l9L4Jnq_T"
   },
   "source": [
    "##### jupyter notebook 단축키\n",
    "\n",
    "- ctrl+enter: 셀 실행   \n",
    "- shift+enter: 셀 실행 및 다음 셀 이동   \n",
    "- alt+enter: 셀 실행, 다음 셀 이동, 새로운 셀 생성\n",
    "- a: 상단에 새로운 셀 만들기\n",
    "- b: 하단에 새로운 셀 만들기\n",
    "- dd: 셀 삭제(x: 셀 삭제)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fr9IowEpnq_U"
   },
   "source": [
    "### 1. 모듈 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9ueLs11Jnq_W"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import time\n",
    "import copy\n",
    "\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt     \n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    mape = np.mean(np.abs((y_true - y_pred)/y_true))*100\n",
    "    return mape\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KL9O-z5Pnq_Y"
   },
   "source": [
    "### 2. 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EdRt1N8LeSOg"
   },
   "outputs": [],
   "source": [
    "random_seed = 2022\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed_all(random_seed) # if use multi-GPU\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y4t-KvVLnq_Z"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('/content/LG_ES_RNN/data/reg_nasdaq100.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qEXesS9Dnq_Z"
   },
   "source": [
    "#### 2.1 Data Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DQxgokeDeSOm",
    "outputId": "fbc72cc7-795f-484b-a5ca-ae24b22dbe1f"
   },
   "outputs": [],
   "source": [
    "x = data.drop(['NDX'], axis=1)\n",
    "y = data[['NDX']]\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, shuffle=False)\n",
    "\n",
    "print(f'x_train shape is {x_train.shape}')\n",
    "print(f'y_train shape is {y_train.shape}')\n",
    "print('-'*35)\n",
    "print(f'x_test shape is {x_test.shape}')\n",
    "print(f'y_test shape is {y_test.shape}')\n",
    "print('-'*35)\n",
    "print('x_train examples')\n",
    "x_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5S9cNof8eSOm",
    "outputId": "2c56ee5f-13d7-4432-855a-065e92135437"
   },
   "outputs": [],
   "source": [
    "x_scaler = MinMaxScaler()\n",
    "y_scaler = MinMaxScaler()\n",
    "\n",
    "columns_list = list(x_train.columns)\n",
    "x_train = x_scaler.fit_transform(x_train)\n",
    "x_test = x_scaler.transform(x_test)\n",
    "y_train = y_scaler.fit_transform(y_train)\n",
    "y_test = y_scaler.transform(y_test)\n",
    "\n",
    "y_train = y_train.squeeze()\n",
    "y_test = y_test.squeeze()\n",
    "\n",
    "print('x_train examples')\n",
    "pd.DataFrame(x_train, columns = columns_list).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xBce3yHMeSOn"
   },
   "source": [
    "#### 2.2 Data Windowing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('/content/LG_ES_RNN/image/image8.JPG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('/content/LG_ES_RNN/image/image9.JPG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OuEKmVEheSOn"
   },
   "outputs": [],
   "source": [
    "def windowing_process(x, y, window_size, shift_size, last = True):\n",
    "    try:\n",
    "        x = x.reset_index(drop=True)\n",
    "        y = y.reset_index(drop=True)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    x_window = []\n",
    "    y_window = []\n",
    "    \n",
    "    for start_idx in range(0, x.shape[0] - window_size + 1, shift_size):\n",
    "        x_window.append(x[start_idx:start_idx + window_size])\n",
    "        \n",
    "        if last == True:\n",
    "            y_window.append(y[start_idx + window_size - 1])\n",
    "        \n",
    "        else:\n",
    "            y_window.append(np.mean(y[start_idx:start_idx + window_size]))\n",
    "    \n",
    "    x_window = np.array(x_window)\n",
    "    y_window = np.array(y_window)\n",
    "    \n",
    "    return x_window, y_window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_yaUNq3DeSOn",
    "outputId": "4fbb18f5-aea0-473b-c838-f95ee3c8e414"
   },
   "outputs": [],
   "source": [
    "x_train, y_train = windowing_process(x_train, y_train, 5, 1, True)\n",
    "x_test, y_test = windowing_process(x_test, y_test, 5, 1, True)\n",
    "\n",
    "print(f'x_train shape is {x_train.shape}')\n",
    "print(f'y_train shape is {y_train.shape}')\n",
    "print('-'*35)\n",
    "print(f'x_test shape is {x_test.shape}')\n",
    "print(f'y_test shape is {y_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "05mADQuZeSOo",
    "outputId": "5a5029db-28af-44c4-fd34-387c56be77eb"
   },
   "outputs": [],
   "source": [
    "n_train = int(0.8 * len(x_train))\n",
    "x_valid, y_valid = x_train[n_train:], y_train[n_train:]\n",
    "x_train, y_train = x_train[:n_train], y_train[:n_train]\n",
    "\n",
    "print(f'x_train shape is {x_train.shape}')\n",
    "print(f'y_train shape is {y_train.shape}')\n",
    "print('-'*35)\n",
    "print(f'x_train shape is {x_valid.shape}')\n",
    "print(f'y_train shape is {y_valid.shape}')\n",
    "print('-'*35)\n",
    "print(f'x_test shape is {x_test.shape}')\n",
    "print(f'y_test shape is {y_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zxG4ygKAnq_e"
   },
   "source": [
    "### 3. RNN Modeling (RNN, LSTM, GRU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "puW9XXHieSOo"
   },
   "source": [
    "#### 3.1 Define the Model Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('/content/LG_ES_RNN/image/image12.JPG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jmJd0HwPnq_f"
   },
   "outputs": [],
   "source": [
    "class RNN_model(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, bidirectional, rnn_type, device='cuda'):\n",
    "        super(RNN_model, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.rnn_type = rnn_type\n",
    "        self.num_directions = 2 if bidirectional == True else 1\n",
    "        self.device = device\n",
    "        \n",
    "        # rnn_type에 따른 recurrent layer 설정\n",
    "        if self.rnn_type == 'rnn':\n",
    "            self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True, bidirectional=bidirectional)\n",
    "        elif self.rnn_type == 'lstm':\n",
    "            self.rnn = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=bidirectional)\n",
    "        elif self.rnn_type == 'gru':\n",
    "            self.rnn = nn.GRU(input_size, hidden_size, num_layers, batch_first=True, bidirectional=bidirectional)\n",
    "        \n",
    "        # bidirectional에 따른 fc layer 구축\n",
    "        # bidirectional 여부에 따라 hidden state의 shape가 달라짐 (True: 2 * hidden_size, False: hidden_size)\n",
    "        self.fc = nn.Linear(self.num_directions * hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # data dimension: (batch_size x input_size x seq_len) -> (batch_size x seq_len x input_size)로 변환\n",
    "        # x = torch.transpose(x, 1, 2)\n",
    "        \n",
    "        # initial hidden states 설정\n",
    "        h0 = torch.zeros(self.num_directions * self.num_layers, x.size(0), self.hidden_size).to(self.device)\n",
    "        \n",
    "        # 선택한 rnn_type의 RNN으로부터 output 도출\n",
    "        if self.rnn_type in ['rnn', 'gru']:\n",
    "            out, _ = self.rnn(x, h0)  # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
    "        else:\n",
    "            # initial cell states 설정\n",
    "            # LSTM의 경우 cell state가 필요\n",
    "            c0 = torch.zeros(self.num_directions * self.num_layers, x.size(0), self.hidden_size).to(self.device)\n",
    "            out, _ = self.rnn(x, (h0, c0))  # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
    "        \n",
    "        out = self.fc(out[:, -1, :]) # 마지막 seq_lengh\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pVnoV9epeSOp"
   },
   "source": [
    "#### 3.2 Define The Training Testing Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hku0LfvoeSOp"
   },
   "outputs": [],
   "source": [
    "class Train_Test():\n",
    "    def __init__(self,  train_loader, valid_loader, test_loader, input_size, device='cuda'): \n",
    "        self.train_loader = train_loader\n",
    "        self.valid_loader = valid_loader\n",
    "        self.test_loader = test_loader\n",
    "\n",
    "        self.input_size = input_size\n",
    "\n",
    "    def train(self, model, dataloaders, criterion, num_epochs, optimizer):\n",
    "        since = time.time() \n",
    "        \n",
    "        train_loss_history = []\n",
    "        val_loss_history = []\n",
    "\n",
    "        best_model_wts = copy.deepcopy(model.state_dict()) # 모델의 초기 Weight값 (각 Layer 별 초기 Weight값이 저장되어 있음)\n",
    "        best_loss = 999999999\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            if epoch == 0 or (epoch + 1) % 10 == 0:\n",
    "                print()\n",
    "                print('Epoch {}/{}'.format(epoch + 1, num_epochs))\n",
    "\n",
    "            # 각 epoch마다 순서대로 training과 validation을 진행\n",
    "            for phase in ['train', 'val']:\n",
    "                if phase == 'train':\n",
    "                    model.train()  # 모델을 training mode로 설정\n",
    "                else:\n",
    "                    model.eval()   # 모델을 validation mode로 설정\n",
    "\n",
    "                running_loss = 0.0\n",
    "                running_total = 0\n",
    "\n",
    "                # training과 validation 단계에 맞는 dataloader에 대하여 학습/검증 진행\n",
    "                for inputs, targets in dataloaders[phase]:\n",
    "                    inputs = inputs.to(device)\n",
    "                    targets = targets.to(device)\n",
    "                    # seq_lens = seq_lens.to(self.parameter['device'])\n",
    "                    \n",
    "                    # parameter gradients를 0으로 설정\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    # forward\n",
    "                    # training 단계에서만 gradient 업데이트 수행\n",
    "                    with torch.set_grad_enabled(phase == 'train'):\n",
    "\n",
    "                        # input을 model에 넣어 output을 도출한 후, loss를 계산함\n",
    "                        outputs = model(inputs)\n",
    "                        outputs = outputs.reshape(-1)\n",
    "                        \n",
    "                        loss = criterion(outputs, targets)\n",
    "\n",
    "                        # backward (optimize): training 단계에서만 수행\n",
    "                        if phase == 'train':\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "\n",
    "                    # batch별 loss를 축적함\n",
    "                    running_loss += loss.item() * inputs.size(0)\n",
    "                    running_total += targets.size(0)\n",
    "\n",
    "                # epoch의 loss 및 accuracy 도출\n",
    "                epoch_loss = running_loss / running_total\n",
    "                                    \n",
    "                if epoch == 0 or (epoch + 1) % 10 == 0:\n",
    "                    print('{} Loss: {:.4f}'.format(phase, epoch_loss))\n",
    "\n",
    "                # validation 단계에서 validation loss가 감소할 때마다 best model 가중치를 업데이트함\n",
    "                if phase == 'val' and epoch_loss < best_loss:\n",
    "                    best_loss = epoch_loss\n",
    "                    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                if phase == 'train':\n",
    "                    train_loss_history.append(epoch_loss)\n",
    "                elif phase == 'val':\n",
    "                    val_loss_history.append(epoch_loss)\n",
    "\n",
    "        # 전체 학습 시간 계산 (학습이 완료된 후)\n",
    "        time_elapsed = time.time() - since\n",
    "        print('\\nTraining complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "        print('Best val MSE: {:4f}'.format(best_loss))\n",
    "\n",
    "        # validation loss가 가장 낮았을 때의 best model 가중치를 불러와 best model을 구축함\n",
    "        model.load_state_dict(best_model_wts)\n",
    "        return model, train_loss_history, val_loss_history\n",
    "\n",
    "    def test(self, model, test_loader):\n",
    "        model.eval()   # 모델을 validation mode로 설정\n",
    "        \n",
    "        # test_loader에 대하여 검증 진행 (gradient update 방지)\n",
    "        with torch.no_grad():\n",
    "\n",
    "            preds = []\n",
    "            y_true = []\n",
    "            for inputs, targets in test_loader:\n",
    "                inputs = inputs.to(device)\n",
    "                targets = targets.to(device)\n",
    "\n",
    "                # forward\n",
    "                # input을 model에 넣어 output을 도출\n",
    "                pred = model(inputs)\n",
    "                \n",
    "                preds.extend(pred.detach().cpu().numpy())\n",
    "                y_true.extend(targets.detach().cpu().numpy())\n",
    "\n",
    "            preds = torch.tensor(preds).reshape(-1)\n",
    "            y_true = torch.tensor(y_true)\n",
    "            \n",
    "            mse = nn.MSELoss()(preds, y_true).item()\n",
    "            preds = preds.detach().cpu().numpy()\n",
    "        return preds, mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Don4rSJGeSOp"
   },
   "source": [
    "### 4. Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "psPvsXXqeSOq"
   },
   "source": [
    "#### RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TNNUP998eSOq"
   },
   "source": [
    "#### 4.1 Hyperparameter Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ATd4L5uzeSOq"
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "input_size = x_train.shape[2]\n",
    "hidden_size = 64\n",
    "num_layers = 2\n",
    "bidirectional = True\n",
    "num_epochs = 200\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else 'cpu') \n",
    "rnn_type='rnn'\n",
    "rnn_best_model_path = f'/content/LG_ES_RNN/ckpt/{rnn_type}_reg.pt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bnwjL-B6eSOq"
   },
   "source": [
    "#### 4.2 Construct Data Loaders and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OX7vA2ideSOq"
   },
   "outputs": [],
   "source": [
    "datasets = []\n",
    "for dataset in [(x_train, y_train), (x_valid, y_valid), (x_test, y_test)]:\n",
    "    x_data = dataset[0]\n",
    "    y_data = dataset[1]\n",
    "    datasets.append(torch.utils.data.TensorDataset(torch.Tensor(x_data), torch.Tensor(y_data)))\n",
    "\n",
    "trainset, validset, testset = datasets[0], datasets[1], datasets[2]\n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(validset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "dataloaders_dict = {'train': train_loader, 'val': valid_loader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tLzCnlk3eSOr"
   },
   "outputs": [],
   "source": [
    "rnn_model = RNN_model(input_size, hidden_size, num_layers, bidirectional, rnn_type, device)\n",
    "rnn_model = rnn_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "otEke8E8eSOr",
    "outputId": "1e9d2abf-d427-48e4-f479-355334c34d2c"
   },
   "outputs": [],
   "source": [
    "rnn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_zpOYKkkeSOr"
   },
   "outputs": [],
   "source": [
    "rnn_trainer = Train_Test(train_loader, valid_loader, test_loader, input_size, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "36UB0fX7eSOr"
   },
   "source": [
    "#### 4.3 Model Training and Save Weights(Parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VCtpRaSZeSOr"
   },
   "source": [
    "#### LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yxX9c-r-eSOr"
   },
   "source": [
    "#### 4.1 Hyperparameter Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rsZxfQx_eSOr"
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "input_size = x_train.shape[2]\n",
    "hidden_size = 64\n",
    "num_layers = 2\n",
    "bidirectional = True\n",
    "num_epochs = 200\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else 'cpu') \n",
    "rnn_type='lstm'\n",
    "lstm_best_model_path = f'/content/LG_ES_RNN/ckpt/{rnn_type}_reg.pt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "USKENAOZeSOs"
   },
   "source": [
    "#### 4.2 Construct Data Loaders and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZOJIHlGYeSOs"
   },
   "outputs": [],
   "source": [
    "lstm_model = RNN_model(input_size, hidden_size, num_layers, bidirectional, rnn_type, device)\n",
    "lstm_model = lstm_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "toqZdZcweSOs",
    "outputId": "a06a4ad2-aa41-4db1-d512-90fde8035f9b"
   },
   "outputs": [],
   "source": [
    "lstm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "33SWLVSheSOs"
   },
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(lstm_model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "74xa8hALeSOs"
   },
   "source": [
    "#### 4.3 Model Training and Save Weights(Parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l9W_U-GbeSOs",
    "outputId": "3b45de7c-8fb3-40a7-f552-84d123b5ae97"
   },
   "outputs": [],
   "source": [
    "lstm_trainer = Train_Test(train_loader, valid_loader, test_loader, input_size, device)\n",
    "lstm_best_model, lstm_train_loss_history, lstm_val_loss_history = lstm_trainer.train(lstm_model, dataloaders_dict, criterion, num_epochs, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iq4a2ewPeSOt",
    "outputId": "badb3020-ef4f-43f6-9211-8654f09de71e"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "plt.title('Loss History (LSTM)')\n",
    "plt.plot(range(num_epochs), lstm_train_loss_history, c='blue', label='Train Loss')\n",
    "plt.plot(range(num_epochs), lstm_val_loss_history, c='red', label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UU2IAbXLeSOt"
   },
   "outputs": [],
   "source": [
    "torch.save(lstm_best_model.state_dict(), lstm_best_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "apMe2HxkeSOt"
   },
   "source": [
    "#### GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4kV-qYYeeSOt"
   },
   "source": [
    "#### 4.1 Hyperparameter Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qZPT2RUreSOt"
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "input_size = x_train.shape[2]\n",
    "hidden_size = 64\n",
    "num_layers = 2\n",
    "bidirectional = True\n",
    "num_epochs = 200\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else 'cpu') \n",
    "rnn_type='gru'\n",
    "gru_best_model_path = f'/content/LG_ES_RNN/ckpt/{rnn_type}_reg.pt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jf1744dneSOt"
   },
   "source": [
    "#### 4.2 Construct Data Loaders and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qOPWmLUxeSOt"
   },
   "outputs": [],
   "source": [
    "gru_model = RNN_model(input_size, hidden_size, num_layers, bidirectional, rnn_type, device)\n",
    "gru_model = gru_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YWTFXw9yeSOu",
    "outputId": "cb9eaa79-d3d9-4bac-cbd6-439714cf565e"
   },
   "outputs": [],
   "source": [
    "gru_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1ZcB3ddLeSOu"
   },
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(gru_model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2EeAvgLeeSOu"
   },
   "source": [
    "#### 4.3 Model Training and Save Weights(Parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EAkRiXimeSOu",
    "outputId": "1a1a7004-38db-4918-f71a-176151079372"
   },
   "outputs": [],
   "source": [
    "gru_trainer = Train_Test(train_loader, valid_loader, test_loader, input_size, device)\n",
    "gru_best_model, gru_train_loss_history, gru_val_loss_history = gru_trainer.train(gru_model, dataloaders_dict, criterion, num_epochs, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oZ99F8O1eSOu",
    "outputId": "d06c7e55-7592-460a-92c1-95cae5562889"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "plt.title('Loss History (GRU)')\n",
    "plt.plot(range(num_epochs), gru_train_loss_history, c='blue', label='Train Loss')\n",
    "plt.plot(range(num_epochs), gru_val_loss_history, c='red', label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L0quQU2zeSOu"
   },
   "outputs": [],
   "source": [
    "torch.save(gru_best_model.state_dict(), gru_best_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O064ByN3eSOv"
   },
   "source": [
    "### 5. Attention Mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6HKbnAA8eSOv"
   },
   "source": [
    "#### 5.1 Define the Model Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('/content/LG_ES_RNN/image/image6.JPG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YUl55lwBeSOv"
   },
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size, device):\n",
    "        super(Attention, self).__init__()\n",
    "        self.device = device\n",
    "        self.hidden_size = hidden_size\n",
    "        self.concat_linear = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.attn = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "\n",
    "    def forward(self, rnn_outputs, final_hidden_state):\n",
    "        # rnn_output.shape:         (batch_size, seq_len, hidden_size)\n",
    "        # final_hidden_state.shape: (batch_size, hidden_size)\n",
    "        \n",
    "        batch_size, seq_len, _ = rnn_outputs.shape\n",
    "        \n",
    "        attn_value = self.attn(rnn_outputs) # (batch_size, seq_len, hidden_dim)\n",
    "        final_hidden_state = final_hidden_state.unsqueeze(2) # (batch_size, hidden_dim, 1)\n",
    "        attn_value = torch.bmm(attn_value, final_hidden_state) # (batch_size, seq_len, 1)\n",
    "        attn_probability = F.softmax(attn_value.squeeze(2), dim=1) # (batch_size, seq_len)\n",
    "\n",
    "        context = torch.bmm(rnn_outputs.transpose(1, 2), attn_probability.unsqueeze(2)).squeeze(2)\n",
    "                 # (batch_size, hidden_size, seq_len) X (batch_size, seq_len, 1) = (batch_size, hidden_size)\n",
    "        attn_hidden = torch.tanh(self.concat_linear(torch.cat((context, final_hidden_state.squeeze(2)), dim=1)))\n",
    "        # attn_hidden = torch.tanh(context)\n",
    "        return attn_hidden, attn_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g6GzcuWfeSOv"
   },
   "outputs": [],
   "source": [
    "class RNN_Attention(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes, bidirectional, rnn_type, device='cuda'):\n",
    "        super(RNN_Attention, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.num_directions = 2 if bidirectional == True else 1\n",
    "        self.rnn_type = rnn_type\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # rnn_type에 따른 recurrent layer 설정\n",
    "        if self.rnn_type == 'rnn':\n",
    "            self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True, bidirectional=bidirectional)\n",
    "        elif self.rnn_type == 'lstm':\n",
    "            self.rnn = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=bidirectional)\n",
    "        elif self.rnn_type == 'gru':\n",
    "            self.rnn = nn.GRU(input_size, hidden_size, num_layers, batch_first=True, bidirectional=bidirectional)\n",
    "        # Attention module활용\n",
    "        self.attn = Attention(hidden_size * self.num_directions, device)\n",
    "        self.fc = nn.Linear(hidden_size * self.num_directions, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, _, seq_len = x.shape\n",
    "        \n",
    "        # data dimension: (batch_size x input_size x seq_len) -> (batch_size x seq_len x input_size)로 변환\n",
    "        # x = torch.transpose(x, 1, 2)\n",
    "        \n",
    "        # initial hidden states 설정\n",
    "        h0 = torch.zeros(self.num_layers * self.num_directions, batch_size, self.hidden_size).to(device)\n",
    "        \n",
    "        # 선택한 rnn_type의 RNN으로부터 output 도출\n",
    "        if self.rnn_type in ['rnn', 'gru']:\n",
    "            out, hidden = self.rnn(x, h0)  # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
    "        else:\n",
    "            # initial cell states 설정\n",
    "            c0 = torch.zeros(self.num_directions * self.num_layers, x.size(0), self.hidden_size).to(self.device)\n",
    "            out, hidden = self.rnn(x, (h0, c0))  # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
    "        \n",
    "        final_state = hidden.view(self.num_layers, self.num_directions, batch_size, self.hidden_size)[-1]\n",
    "        \n",
    "        # Handle directions\n",
    "        final_hidden_state = None\n",
    "        if self.num_directions == 1:\n",
    "            final_hidden_state = final_state.squeeze(0)\n",
    "        elif self.num_directions == 2:\n",
    "            h_1, h_2 = final_state[0], final_state[1]\n",
    "            final_hidden_state = torch.cat((h_1, h_2), 1)  # Concatenate both states\n",
    "\n",
    "        # Push through attention layer\n",
    "        attn_output, attn_scores = self.attn(out, final_hidden_state)\n",
    "        attn_output = self.fc(attn_output)\n",
    "        return attn_output, attn_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fUC0oSExeSOv"
   },
   "source": [
    "#### 5.2 Define The Training Testing Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_y3MJ3VreSOv"
   },
   "outputs": [],
   "source": [
    "class Train_Test_Attention():\n",
    "    def __init__(self,  train_loader, valid_loader, test_loader, input_size, num_classes, device='cuda'): ##### config는 jupyter 파일을 참고\n",
    "        \"\"\"\n",
    "        Initialize Train_Test class\n",
    "\n",
    "        :param config: configuration\n",
    "        :type config: dictionary\n",
    "\n",
    "        :param train_loader: train dataloader\n",
    "        :type config: DataLoader\n",
    "\n",
    "        :param valid_loader: validation dataloader\n",
    "        :type config: DataLoader\n",
    "\n",
    "        :param test_loader: test dataloader\n",
    "        :type config: DataLoader\n",
    "        \"\"\"\n",
    "\n",
    "        self.train_loader = train_loader\n",
    "        self.valid_loader = valid_loader\n",
    "        self.test_loader = test_loader\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def train(self, model, dataloaders, criterion, num_epochs, optimizer):\n",
    "        \"\"\"\n",
    "        Train the model\n",
    "\n",
    "        :param model: initialized model\n",
    "        :type model: model\n",
    "\n",
    "        :param dataloaders: train & validation dataloaders\n",
    "        :type dataloaders: dictionary\n",
    "\n",
    "        :param criterion: loss function for training\n",
    "        :type criterion: criterion\n",
    "\n",
    "        :param num_epochs: the number of train epochs\n",
    "        :type num_epochs: int\n",
    "\n",
    "        :param optimizer: optimizer used in training\n",
    "        :type optimizer: optimizer\n",
    "\n",
    "        :return: trained model\n",
    "        :rtype: model\n",
    "        \"\"\"\n",
    "\n",
    "        since = time.time() \n",
    "        \n",
    "        train_loss_history = []\n",
    "        val_loss_history = []\n",
    "\n",
    "        best_model_wts = copy.deepcopy(model.state_dict()) # 모델의 초기 Weight값 (각 Layer 별 초기 Weight값이 저장되어 있음)\n",
    "        best_loss = 999999999\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            if epoch == 0 or (epoch + 1) % 10 == 0:\n",
    "                print()\n",
    "                print('Epoch {}/{}'.format(epoch + 1, num_epochs))\n",
    "\n",
    "            # 각 epoch마다 순서대로 training과 validation을 진행\n",
    "            for phase in ['train', 'val']:\n",
    "                if phase == 'train':\n",
    "                    model.train()  # 모델을 training mode로 설정\n",
    "                else:\n",
    "                    model.eval()   # 모델을 validation mode로 설정\n",
    "\n",
    "                running_loss = 0.0\n",
    "                running_total = 0\n",
    "\n",
    "                # training과 validation 단계에 맞는 dataloader에 대하여 학습/검증 진행\n",
    "                for inputs, targets in dataloaders[phase]:\n",
    "                    inputs = inputs.to(device)\n",
    "                    targets = targets.to(device)\n",
    "                    # seq_lens = seq_lens.to(self.parameter['device'])\n",
    "                    \n",
    "                    # parameter gradients를 0으로 설정\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    # forward\n",
    "                    # training 단계에서만 gradient 업데이트 수행\n",
    "                    with torch.set_grad_enabled(phase == 'train'):\n",
    "\n",
    "                        # input을 model에 넣어 output을 도출한 후, loss를 계산함\n",
    "                        outputs, attn_scores = model(inputs)\n",
    "                        outputs = outputs.reshape(-1)\n",
    "                        \n",
    "                        loss = criterion(outputs, targets)\n",
    "\n",
    "                        # backward (optimize): training 단계에서만 수행\n",
    "                        if phase == 'train':\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "\n",
    "                    # batch별 loss를 축적함\n",
    "                    running_loss += loss.item() * inputs.size(0)\n",
    "                    running_total += targets.size(0)\n",
    "\n",
    "                # epoch의 loss 및 accuracy 도출\n",
    "                epoch_loss = running_loss / running_total\n",
    "                                    \n",
    "                if epoch == 0 or (epoch + 1) % 10 == 0:\n",
    "                    print('{} Loss: {:.4f}'.format(phase, epoch_loss))\n",
    "\n",
    "                # validation 단계에서 validation loss가 감소할 때마다 best model 가중치를 업데이트함\n",
    "                if phase == 'val' and epoch_loss < best_loss:\n",
    "                    best_loss = epoch_loss\n",
    "                    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                if phase == 'train':\n",
    "                    train_loss_history.append(epoch_loss)\n",
    "                elif phase == 'val':\n",
    "                    val_loss_history.append(epoch_loss)\n",
    "\n",
    "        # 전체 학습 시간 계산 (학습이 완료된 후)\n",
    "        time_elapsed = time.time() - since\n",
    "        print('\\nTraining complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "        print('Best val MSE: {:4f}'.format(best_loss))\n",
    "\n",
    "        # validation loss가 가장 낮았을 때의 best model 가중치를 불러와 best model을 구축함\n",
    "        model.load_state_dict(best_model_wts)\n",
    "        return model, train_loss_history, val_loss_history, attn_scores\n",
    "\n",
    "    def test(self, model, test_loader):\n",
    "        \"\"\"\n",
    "        Predict classes for test dataset based on the trained model\n",
    "\n",
    "        :param model: best trained model\n",
    "        :type model: model\n",
    "\n",
    "        :param test_loader: test dataloader\n",
    "        :type test_loader: DataLoader\n",
    "\n",
    "        :return: predicted classes\n",
    "        :rtype: numpy array\n",
    "\n",
    "        :return: prediction probabilities\n",
    "        :rtype: numpy array\n",
    "\n",
    "        :return: test accuracy\n",
    "        :rtype: float\n",
    "        \"\"\"\n",
    "\n",
    "        model.eval()   # 모델을 validation mode로 설정\n",
    "        \n",
    "        # test_loader에 대하여 검증 진행 (gradient update 방지)\n",
    "        with torch.no_grad():\n",
    "\n",
    "            preds = []\n",
    "            y_true = []\n",
    "            attn_scores = []\n",
    "            for inputs, targets in test_loader:\n",
    "                inputs = inputs.to(device)\n",
    "                targets = targets.to(device)\n",
    "\n",
    "                # forward\n",
    "                # input을 model에 넣어 output을 도출\n",
    "                pred, attn_score = model(inputs)\n",
    "                \n",
    "                preds.extend(pred.detach().cpu().numpy())\n",
    "                y_true.extend(targets.detach().cpu().numpy())\n",
    "                attn_scores.extend(attn_score.detach().cpu().tolist())\n",
    "                \n",
    "            preds = torch.tensor(preds).reshape(-1)\n",
    "            y_true = torch.tensor(y_true)\n",
    "            attn_scores = torch.tensor(attn_scores)\n",
    "            \n",
    "            mse = nn.MSELoss()(preds, y_true).item()\n",
    "            preds = preds.detach().cpu().numpy()\n",
    "            attn_scores = attn_scores.detach().cpu().numpy()\n",
    "        return preds, mse, attn_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UZligejgeSOw"
   },
   "source": [
    "#### 5.3 Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4icvOubMeSOw"
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "input_size = x_train.shape[2]\n",
    "hidden_size = 64\n",
    "num_layers = 2\n",
    "bidirectional = True\n",
    "num_epochs = 500\n",
    "num_classes = 1\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else 'cpu') \n",
    "rnn_type='rnn'\n",
    "attn_best_model_path = f'/content/LG_ES_RNN/ckpt/{rnn_type}_with_attn_reg.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qaWPXJT6eSOx"
   },
   "outputs": [],
   "source": [
    "attn_model = RNN_Attention(input_size, hidden_size, num_layers, num_classes, bidirectional, rnn_type, device)\n",
    "attn_model = attn_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X3_tN1vReSOx",
    "outputId": "779cfaa4-fe0f-457a-9ff7-2834c7d88cf6"
   },
   "outputs": [],
   "source": [
    "attn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E37BIlnbeSOx"
   },
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(attn_model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "telGVE3VeSOx",
    "outputId": "9be90c4b-49ad-4531-da88-c06446be9c90"
   },
   "outputs": [],
   "source": [
    "attn_trainer = Train_Test_Attention(train_loader, valid_loader, test_loader, input_size, device)\n",
    "attn_best_model, attn_train_loss_history, attn_val_loss_history, attn_scores = attn_trainer.train(attn_model, dataloaders_dict, criterion, num_epochs, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1AGI62bReSOx",
    "outputId": "9726d1db-c7c7-419d-f0b7-b37ac93d2127"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "plt.title('Loss History (Attention)')\n",
    "plt.plot(range(num_epochs), attn_train_loss_history, c='blue', label='Train Loss')\n",
    "plt.plot(range(num_epochs), attn_val_loss_history, c='red', label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_kEcqRH5eSOx"
   },
   "outputs": [],
   "source": [
    "torch.save(attn_best_model.state_dict(), attn_best_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XC9pu5Y5eSOy"
   },
   "source": [
    "### 6. Model Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4I67ywK_eSOy"
   },
   "source": [
    "#### 6.1 Load Model Weights(Parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5bAYkL5OeSOy",
    "outputId": "8ac40e7c-bfa1-40db-9f62-1e3eaeb9ce0c"
   },
   "outputs": [],
   "source": [
    "rnn_model.load_state_dict(torch.load(rnn_best_model_path))\n",
    "lstm_model.load_state_dict(torch.load(lstm_best_model_path))\n",
    "gru_model.load_state_dict(torch.load(gru_best_model_path))\n",
    "attn_model.load_state_dict(torch.load(attn_best_model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9efZpN3TeSOy"
   },
   "outputs": [],
   "source": [
    "def regression_report(y_true, y_pred):\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    mae = mean_absolute_error(y_true, y_pred) \n",
    "    mse = mean_squared_error(y_true, y_pred) \n",
    "    rmse = np.sqrt(mse)\n",
    "    mape = mean_absolute_percentage_error(y_true, y_pred)\n",
    "      \n",
    "    print('The regression reports are as follows:')\n",
    "    print('r2: ', round(r2,4))\n",
    "    print('MAE: ', round(mae,4))\n",
    "    print('MSE: ', round(mse,4))\n",
    "    print('RMSE: ', round(rmse,4))\n",
    "    print('MAPE: ', round(mape,4))\n",
    "    return r2, mae, mse, rmse, mape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mz_2HdDxeSOy"
   },
   "outputs": [],
   "source": [
    "y_test_inverse = y_scaler.inverse_transform(pd.DataFrame(y_test))\n",
    "y_train_inverse = y_scaler.inverse_transform(pd.DataFrame(y_train))\n",
    "y_valid_inverse = y_scaler.inverse_transform(pd.DataFrame(y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iplKU6d5eSOy",
    "outputId": "2c436055-7d27-4d4e-df10-a3488219066d"
   },
   "outputs": [],
   "source": [
    "name_list = []\n",
    "r2_list = []\n",
    "mae_list = []\n",
    "mse_list = []\n",
    "rmse_list = []\n",
    "mape_list = []\n",
    "\n",
    "results_df = pd.DataFrame()\n",
    "y_pred_df = pd.DataFrame()\n",
    "    \n",
    "for model_set in zip(['rnn', 'lstm', 'gru', 'attn'], \n",
    "                      [rnn_model, lstm_model, gru_model, attn_model], \n",
    "                      [rnn_trainer, lstm_trainer, gru_trainer, attn_trainer]):\n",
    "    name = model_set[0]\n",
    "    model = model_set[1]\n",
    "    trainer = model_set[2]\n",
    "    print(name)\n",
    "    try:\n",
    "        y_pred, mse = trainer.test(model, test_loader)\n",
    "    except:\n",
    "        y_pred, mse, attn_scores = trainer.test(model, test_loader)\n",
    "        \n",
    "    y_pred_inverse = y_scaler.inverse_transform(pd.DataFrame(y_pred))   \n",
    "    r2, mae, mse, rmse, mape = regression_report (y_test_inverse.flatten(), y_pred_inverse.flatten())\n",
    "    r2_list.append(r2)\n",
    "    mae_list.append(mae)\n",
    "    mse_list.append(mse)\n",
    "    rmse_list.append(rmse)\n",
    "    mape_list.append(mape)\n",
    "    name_list.append(name)\n",
    "    \n",
    "    y_train_concat = np.concatenate([y_train_inverse, y_valid_inverse], axis=0)\n",
    "    y_train = np.concatenate([y_train_concat, np.array([np.nan]*len(y_test_inverse)).reshape(len(y_test_inverse), -1)], axis=0)\n",
    "    y_test = np.concatenate([np.array([np.nan]*len(y_train_concat)).reshape(len(y_train_concat), -1), y_test_inverse], axis=0)\n",
    "    y_pred = np.concatenate([np.array([np.nan]*len(y_train_concat)).reshape(len(y_train_concat), -1), y_pred_inverse], axis=0)\n",
    "\n",
    "    y_train = pd.DataFrame(y_train, columns = [f'Train set'])\n",
    "    y_test = pd.DataFrame(y_test, columns = [f'Test set'])\n",
    "    y_pred = pd.DataFrame(y_pred, columns = [f'Predictions {name}'])\n",
    "    y_pred_df = pd.concat([y_pred_df, y_pred], axis=1)\n",
    "    \n",
    "results_df['Model'] = name_list\n",
    "results_df['R2 score'] = r2_list\n",
    "results_df['MAE'] = mae_list\n",
    "results_df['MSE'] = mse_list\n",
    "results_df['RMSE'] = rmse_list\n",
    "results_df['MAPE'] = mape_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z-McJrYKeSOz",
    "outputId": "49a97d47-3bd4-4c22-90a8-9ffd1f50a943"
   },
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7FQpeXO9eSOz",
    "outputId": "b01e9fb9-b9a7-4881-e1c7-61974e5a49db"
   },
   "outputs": [],
   "source": [
    "y_pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lnZDB88heSOz",
    "outputId": "586f1f82-0ce0-4df3-9f3f-223cd5cdf91f"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,6))\n",
    "plt.title('RNN regression results', fontsize=12)\n",
    "plt.xlabel('Date', fontsize=12)\n",
    "plt.ylabel('NASDAQ 100 Index', fontsize=12)\n",
    "plt.axvline(x=len(y_train_concat), color='r', label='Start Prediction', ls='--')\n",
    "plt.plot(y_train['Train set'])\n",
    "plt.plot(y_test['Test set'])\n",
    "plt.plot(y_pred_df['Predictions rnn'])\n",
    "plt.plot(y_pred_df['Predictions lstm'])\n",
    "plt.plot(y_pred_df['Predictions gru'])\n",
    "plt.plot(y_pred_df['Predictions attn'])\n",
    "plt.legend(['Start Prediction', 'Train set', 'Test set', \n",
    "            'Predictions (RNN)', 'Predictions (LSTM)',\n",
    "            'Predictions (GRU)', 'Predictions (Attention)'], loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gqBHwh5-nq_q"
   },
   "source": [
    "# EOD"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "4_RNN_regression_comparative_results.ipynb",
   "provenance": [
    {
     "file_id": "https://github.com/Jiyoon52/LG_ES_RNN/blob/main/4_RNN_regression_comparative_results.ipynb",
     "timestamp": 1656959349376
    },
    {
     "file_id": "https://github.com/Jiyoon52/LG_time_series_day02_dataset/blob/main/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%EC%8B%9C%EA%B3%84%EC%97%B4%EB%B6%84%EC%84%9D%202%ED%9A%8C%EC%B0%A8%20-%201.ipynb",
     "timestamp": 1642046886935
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "toc": {
   "base_numbering": "1",
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "357.448px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
