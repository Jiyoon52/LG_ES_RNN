{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F2eg6OU7eZKP",
    "outputId": "fc80ea17-f37e-47ad-9006-f6e75152495f"
   },
   "outputs": [],
   "source": [
    "!pip install IPython\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jXS5MqJNeZKR",
    "outputId": "facf067f-3106-44ec-d738-4d762624e9df"
   },
   "outputs": [],
   "source": [
    "!apt install python3.7\n",
    "!pip install -U torchtext==0.8.1\n",
    "\n",
    "# 문장을 토큰화하는 모듈 설치\n",
    "!python -m spacy download en_core_web_sm\n",
    "!python -m spacy download de_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jAM51Z7znq_V"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/Jiyoon52/LG_ES_RNN.git # colab 사용시"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9uXbP9sNnq_S"
   },
   "source": [
    "# [Sequence to Sequence - many to many] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QL7l9L4Jnq_T"
   },
   "source": [
    "##### jupyter notebook 단축키\n",
    "\n",
    "- ctrl+enter: 셀 실행   \n",
    "- shift+enter: 셀 실행 및 다음 셀 이동   \n",
    "- alt+enter: 셀 실행, 다음 셀 이동, 새로운 셀 생성\n",
    "- a: 상단에 새로운 셀 만들기\n",
    "- b: 하단에 새로운 셀 만들기\n",
    "- dd: 셀 삭제(x: 셀 삭제)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('/content/LG_ES_RNN/image/image13.JPG')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fr9IowEpnq_U"
   },
   "source": [
    "### 1. 모듈 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9ueLs11Jnq_W"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "\n",
    "from torchtext.datasets import TranslationDataset, Multi30k\n",
    "from torchtext.data import Field, BucketIterator\n",
    "\n",
    "import spacy\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "\n",
    "import torchtext\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from collections import Counter\n",
    "from torchtext.vocab import Vocab\n",
    "from torchtext.utils import download_from_url, extract_archive\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "import io\n",
    "\n",
    "import random\n",
    "from typing import Tuple\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KL9O-z5Pnq_Y"
   },
   "source": [
    "### 2. 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vjnrikOSeZKU"
   },
   "outputs": [],
   "source": [
    "random_seed = 2022\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed_all(random_seed) # if use multi-GPU\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://www.statmt.org/wmt16/multimodal-task.html#task1\n",
    "- 독일어-영어 번역 작업을 위해 제공하는 데이터셋\n",
    "- DE: 독일어, EN: 영어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y4t-KvVLnq_Z"
   },
   "outputs": [],
   "source": [
    "url_base = 'https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/raw/'\n",
    "train_urls = ('train.de.gz', 'train.en.gz')\n",
    "val_urls = ('val.de.gz', 'val.en.gz')\n",
    "test_urls = ('test_2016_flickr.de.gz', 'test_2016_flickr.en.gz')\n",
    "\n",
    "train_filepaths = [extract_archive(download_from_url(url_base + url))[0] for url in train_urls]\n",
    "val_filepaths = [extract_archive(download_from_url(url_base + url))[0] for url in val_urls]\n",
    "test_filepaths = [extract_archive(download_from_url(url_base + url))[0] for url in test_urls]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- tokenizers는 문장을 개별 token으로 변환해주는 데 사용\n",
    "  - e.g. \"I love you!\" --> [\"I\", \"love\", \"you\", \"!\"]\n",
    "- nlp를 쉽게 할 수 있도록 도와주는 python package인 `spaCy`를 이용하여, token화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_tokenizer = get_tokenizer('spacy', language='de_core_news_sm')\n",
    "en_tokenizer = get_tokenizer('spacy', language='en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- url로부터 데이터를 부르고, tokenizing한뒤 구성된 단어로 vocab(단어 군) 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-mAj4KDieZKU"
   },
   "outputs": [],
   "source": [
    "def build_vocab(filepath, tokenizer):\n",
    "  counter = Counter()\n",
    "  with io.open(filepath, encoding=\"utf8\") as f:\n",
    "    for string_ in f:\n",
    "      counter.update(tokenizer(string_))\n",
    "  return Vocab(counter, specials=['<unk>', '<pad>', '<bos>', '<eos>'])\n",
    "\n",
    "de_vocab = build_vocab(train_filepaths[0], de_tokenizer)\n",
    "en_vocab = build_vocab(train_filepaths[1], en_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- vocab(단어 군)에서의 index를 활용하여 각 단어(토큰)에 숫자를 할당"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T06kbquBeZKV"
   },
   "outputs": [],
   "source": [
    "def data_process(filepaths):\n",
    "  raw_de_iter = iter(io.open(filepaths[0], encoding=\"utf8\"))\n",
    "  raw_en_iter = iter(io.open(filepaths[1], encoding=\"utf8\"))\n",
    "  data = []\n",
    "  for (raw_de, raw_en) in zip(raw_de_iter, raw_en_iter):\n",
    "    de_tensor_ = torch.tensor([de_vocab[token] for token in de_tokenizer(raw_de)],\n",
    "                            dtype=torch.long)\n",
    "    en_tensor_ = torch.tensor([en_vocab[token] for token in en_tokenizer(raw_en)],\n",
    "                            dtype=torch.long)\n",
    "    data.append((de_tensor_, en_tensor_))\n",
    "  return data\n",
    "\n",
    "train_data = data_process(train_filepaths)\n",
    "val_data = data_process(val_filepaths)\n",
    "test_data = data_process(test_filepaths)\n",
    "\n",
    "# 각 set을 구성하는 문장 pair의 개수를 의미\n",
    "print(f'test_data length is {len(test_data)}')\n",
    "print(f'val_data length is {len(val_data)}')\n",
    "print(f'test_data length is {len(test_data)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 각 pair에 대한 예시 확인해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "otqg7TxbeZKV"
   },
   "outputs": [],
   "source": [
    "train_data[0][0]\n",
    "train_data[0][1]\n",
    "\n",
    "de_itos = de_vocab.itos\n",
    "en_itos = en_vocab.itos\n",
    "\n",
    "exam_de = ([de_itos[i] for i in train_data[0][0]])\n",
    "exam_en = ([en_itos[i] for i in train_data[0][1]])\n",
    "\n",
    "print('Germany sentence')\n",
    "print(exam_de)\n",
    "\n",
    "print('English sentence')\n",
    "print(exam_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('/content/LG_ES_RNN/image/image14.JPG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a5UFu2oSeZKV"
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "PAD_IDX = de_vocab['<pad>'] # padding token\n",
    "BOS_IDX = de_vocab['<bos>'] # begin of sentence token\n",
    "EOS_IDX = de_vocab['<eos>'] # end of sentence token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SZ9qFYpMeZKV"
   },
   "outputs": [],
   "source": [
    "def generate_batch(data_batch):\n",
    "  de_batch, en_batch = [], []\n",
    "  for (de_item, en_item) in data_batch:\n",
    "    de_batch.append(torch.cat([torch.tensor([BOS_IDX]), de_item, torch.tensor([EOS_IDX])], dim=0))\n",
    "    en_batch.append(torch.cat([torch.tensor([BOS_IDX]), en_item, torch.tensor([EOS_IDX])], dim=0))\n",
    "  de_batch = pad_sequence(de_batch, padding_value=PAD_IDX)\n",
    "  en_batch = pad_sequence(en_batch, padding_value=PAD_IDX)\n",
    "  return de_batch, en_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U9W8oimleZKW"
   },
   "outputs": [],
   "source": [
    "train_iter = DataLoader(train_data, batch_size,\n",
    "                        shuffle=True, collate_fn=generate_batch)\n",
    "valid_iter = DataLoader(val_data, batch_size,\n",
    "                        shuffle=True, collate_fn=generate_batch)\n",
    "test_iter = DataLoader(test_data, batch_size,\n",
    "                       shuffle=True, collate_fn=generate_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zxG4ygKAnq_e"
   },
   "source": [
    "### 3. Seq2Seq Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EvYMON05eZKW"
   },
   "source": [
    "#### 3.1 Define the Model Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('/content/LG_ES_RNN/image/image15.JPG')\n",
    "# 참고: https://deep-learning-study.tistory.com/686"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoder는 입력 senquence를 입력받아 encode하여 고정된 크기의 context vector를 생성하며, \n",
    "입력 cell과 입력 hidden state는 zero tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jmJd0HwPnq_f"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        # embedding: 입력값을 emd_dim 벡터로 변경\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        \n",
    "        # embedding을 입력받아 hid_dim 크기의 hidden state, cell 출력\n",
    "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src):\n",
    "        # src = [src len, batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        # embedded = [src len, batch size, emb dim]\n",
    "        \n",
    "        outputs, (hidden, cell) = self.rnn(embedded)\n",
    "        # outputs = [src len, batch size, hid dim * n directions]\n",
    "        # hidden = [n layers * n directions, batch size, hid dim]\n",
    "        # cell = [n layers * n directions, batch size, hid dim]\n",
    "        \n",
    "        # outputs are always from the top hidden layer\n",
    "        \n",
    "        return hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('/content/LG_ES_RNN/image/image16.JPG')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decoder은 encode된 context vector를 입력받아 decode하여 단어를 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.output_dim = output_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        # content vector(output_dim)를 입력받아 emb_dim 출력\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        \n",
    "        # embedding을 입력받아 hid_dim 크기의 hidden state, cell 출력\n",
    "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout)\n",
    "        \n",
    "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input, hidden, cell):\n",
    "        \n",
    "        # input = [batch size]\n",
    "        # hidden = [n layers * n directions, batch size, hid dim]\n",
    "        # cell = [n layers * n directions, batch size, hid dim]\n",
    "        \n",
    "        # n directions in the decoder will both always be 1, therefore:\n",
    "        # hidden = [n layers, batch size, hid dim]\n",
    "        # context = [n layers, batch size, hid dim]\n",
    "        \n",
    "        input = input.unsqueeze(0)\n",
    "        # input = [1, batch size]\n",
    "        # 하나의 token씩 decoding, 첫번째 input은  <bos>\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        \n",
    "        # embedded = [1, batch size, emb dim]\n",
    "                \n",
    "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
    "        \n",
    "        # output = [seq len, batch size, hid dim * n directions]\n",
    "        # hidden = [n layers * n directions, batch size, hid dim]\n",
    "        # cell = [n layers * n directions, batch size, hid dim]\n",
    "        \n",
    "        # seq len and n directions will always be 1 in the decoder, therefore:\n",
    "        # output = [1, batch size, hid dim]\n",
    "        # hidden = [n layers, batch size, hid dim]\n",
    "        # cell = [n layers, batch size, hid dim]\n",
    "        \n",
    "        prediction = self.fc_out(output.squeeze(0))\n",
    "        \n",
    "        #prediction = [batch size, output dim]\n",
    "        \n",
    "        return prediction, hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C3hUMUXyeZKW"
   },
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        \n",
    "        assert encoder.hid_dim == decoder.hid_dim, \\\n",
    "            \"Hidden dimensions of encoder and decoder must be equal!\"\n",
    "        assert encoder.n_layers == decoder.n_layers, \\\n",
    "            \"Encoder and decoder must have equal number of layers!\"\n",
    "        \n",
    "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n",
    "        \n",
    "        # src = [src len, batch size]\n",
    "        # trg = [trg len, batch size]\n",
    "        # teacher_forcing_ratio is probability to use teacher forcing\n",
    "        # teacher forcing은 다음 입력으로 디코더의 예측을 사용하는 대신 실제 목표 출력을 다음 입력으로 사용\n",
    "        \n",
    "        # e.g. if teacher_forcing_ratio is 0.75 we use ground-truth inputs 75% of the time\n",
    "        \n",
    "        batch_size = trg.shape[1]\n",
    "        trg_len = trg.shape[0] # 타겟 토큰 길이 얻기\n",
    "        trg_vocab_size = self.decoder.output_dim # context vector의 차원\n",
    "        \n",
    "        #tensor to store decoder outputs\n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
    "        \n",
    "        #last hidden state of the encoder is used as the initial hidden state of the decoder\n",
    "        hidden, cell = self.encoder(src)\n",
    "        \n",
    "        #first input to the decoder is the <sos> tokens\n",
    "        input = trg[0,:]\n",
    "        \n",
    "        for t in range(1, trg_len):\n",
    "            \n",
    "            #insert input token embedding, previous hidden and previous cell states\n",
    "            #receive output tensor (predictions) and new hidden and cell states\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
    "            \n",
    "            #place predictions in a tensor holding predictions for each token\n",
    "            outputs[t] = output\n",
    "            \n",
    "            #decide if we are going to use teacher forcing or not\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            \n",
    "            #get the highest predicted token from our predictions\n",
    "            top1 = output.argmax(1) \n",
    "            \n",
    "            #if teacher forcing, use actual next token as next input\n",
    "            #if not, use predicted token\n",
    "            input = trg[t] if teacher_force else top1\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- encoder에 source(input) sentence를 입력\n",
    "- encoder를 학습시켜 고정된 크기의 context vector를 출력\n",
    "- context vector를 decoder에 넣어 예측된 target(output) sentence를 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b5dlh2ojeZKX"
   },
   "source": [
    "#### 3.2 Define The Training Testing Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XMh3ENK1eZKX"
   },
   "outputs": [],
   "source": [
    "def train(model: nn.Module,\n",
    "          iterator: torch.utils.data.DataLoader,\n",
    "          optimizer: optim.Optimizer,\n",
    "          criterion: nn.Module,\n",
    "          clip: float):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for _, (src, trg) in enumerate(iterator):\n",
    "        src, trg = src.to(device), trg.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(src, trg)\n",
    "\n",
    "        output = output[1:].view(-1, output.shape[-1])\n",
    "        trg = trg[1:].view(-1)\n",
    "\n",
    "        loss = criterion(output, trg)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "\n",
    "def evaluate(model: nn.Module,\n",
    "             iterator: torch.utils.data.DataLoader,\n",
    "             criterion: nn.Module):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    epoch_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for _, (src, trg) in enumerate(iterator):\n",
    "            src, trg = src.to(device), trg.to(device)\n",
    "\n",
    "            output = model(src, trg, 0) #turn off teacher forcing\n",
    "\n",
    "            output = output[1:].view(-1, output.shape[-1])\n",
    "            trg = trg[1:].view(-1)\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(iterator), output\n",
    "  \n",
    "def epoch_time(start_time: int,\n",
    "               end_time: int):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wM1f_xMdeZKY"
   },
   "source": [
    "### 4. Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0a6w91s2eZKY"
   },
   "source": [
    "#### 4.1 Hyperparameter Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SckrBRcLeZKY"
   },
   "outputs": [],
   "source": [
    "input_dim = len(de_vocab)\n",
    "output_dim = len(en_vocab)\n",
    "enc_emb_dim = 256\n",
    "dec_emb_dim = 256\n",
    "hid_dim = 512\n",
    "n_layers = 2\n",
    "enc_dropout = 0.5\n",
    "dec_dropout = 0.5\n",
    "num_epochs = 10\n",
    "clip = 1\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else 'cpu') \n",
    "best_model_path = '/content/LG_ES_RNN/ckpt/seq2seq.pt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LKUkOsEeeZKZ"
   },
   "source": [
    "#### 4.2 Construct Data Loaders and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wFBRXRq-eZKZ"
   },
   "outputs": [],
   "source": [
    "enc = Encoder(input_dim, enc_emb_dim, hid_dim, n_layers, enc_dropout)\n",
    "dec = Decoder(output_dim, dec_emb_dim, hid_dim, n_layers, dec_dropout)\n",
    "\n",
    "model = Seq2Seq(enc, dec, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rR4vPbh8eZKZ"
   },
   "outputs": [],
   "source": [
    "# 가중치 초기화\n",
    "def init_weights(m: nn.Module):\n",
    "    for name, param in m.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
    "        else:\n",
    "            nn.init.constant_(param.data, 0)\n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DMCDrcNFeZKa"
   },
   "outputs": [],
   "source": [
    "# 모델의 학습대상인 파라미터 수 측정\n",
    "def count_parameters(model: nn.Module):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nRLTQOW7eZKa"
   },
   "source": [
    "#### 4.3 Model Training and Save Weights(Parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xVaxvSd3eZKa"
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())\n",
    "PAD_IDX = en_vocab.stoi['<pad>']\n",
    "\n",
    "# <pad> token에 해당하는 index는 무시\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EyPYpMiteZKa",
    "outputId": "e019dc45-4251-4244-fbfd-127beab405ee"
   },
   "outputs": [],
   "source": [
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    train_loss = train(model, train_iter, optimizer, criterion, clip)\n",
    "    valid_loss, _ = evaluate(model, valid_iter, criterion)\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f}')\n",
    "\n",
    "test_loss, pred = evaluate(model, test_iter, criterion)\n",
    "torch.save(model.state_dict(), best_model_path)\n",
    "print(f'| Test Loss: {test_loss:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hfvPo_VreZKa"
   },
   "source": [
    "### 5. Model Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w_ZdPjMfeZKa"
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(best_model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fJMhU_MeeZKa",
    "outputId": "da38f481-5924-4b9b-d93f-d71f12943bf0"
   },
   "outputs": [],
   "source": [
    "test_loss, pred_output = evaluate(model, test_iter, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gqBHwh5-nq_q"
   },
   "source": [
    "# EOD"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "5_Seq2Seq.ipynb",
   "provenance": [
    {
     "file_id": "https://github.com/Jiyoon52/LG_ES_RNN/blob/main/5_Seq2Seq.ipynb",
     "timestamp": 1656959377536
    },
    {
     "file_id": "https://github.com/Jiyoon52/LG_time_series_day02_dataset/blob/main/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%EC%8B%9C%EA%B3%84%EC%97%B4%EB%B6%84%EC%84%9D%202%ED%9A%8C%EC%B0%A8%20-%201.ipynb",
     "timestamp": 1642046886935
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "toc": {
   "base_numbering": "1",
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "357.448px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
